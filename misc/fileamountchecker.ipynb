{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imageio\n",
    "# imageio.plugins.ffmpeg.download()\n",
    "import numpy as np\n",
    "import os\n",
    "#from inceptionresnetv2 import inceptionresnetv2\n",
    "# from torchvision.models import resnet101\n",
    "import pretrainedmodels\n",
    "model_name = 'inceptionresnetv2'\n",
    "import torchvision.transforms as trn\n",
    "import torch\n",
    "import argparse\n",
    "import process_anno\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "class Identity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feats(file_path, filenames, frame_num, batch_size, save_path, anno):\n",
    "    \"\"\"Extract 2D features (saved in .npy) for frames in a video.\"\"\"\n",
    "    net = pretrainedmodels.__dict__[model_name](num_classes=1001, pretrained='imagenet+background')\n",
    "    # net = resnet101(pretrained=True)\n",
    "    net.last_linear = Identity()\n",
    "    # net = inceptionresnetv2(num_classes=1001, pretrained='imagenet+background')\n",
    "\n",
    "    net.eval()\n",
    "    net.cuda()\n",
    "    transform = trn.Compose([trn.ToPILImage(),\n",
    "        trn.Resize((299, 299)), # 299 for IRV2 # 224 for ResNet\n",
    "        trn.ToTensor(),\n",
    "        trn.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])])#trn.Normalize(net.mean, net.std)])\n",
    "        \n",
    "    # print(\"res101 Network loaded\")\n",
    "    print(\"inceptionresnetv2 Network loaded\")\t\n",
    "\n",
    "    #Read videos and extract features in batches\n",
    "    cnt = 0\n",
    "    for fname in tqdm(filenames):\n",
    "        # start / end time list\n",
    "        # print(fname)\n",
    "        # if fname[:-4] in list(anno.keys()):\n",
    "        # \tbd_info = anno[fname[:-4]]\n",
    "        # else:\n",
    "        # \tcontinue\n",
    "        bd_info = anno[fname]['segment']\n",
    "        for bd_ in bd_info:\n",
    "            \n",
    "            # get each set of start / end time form list\n",
    "            # cnt = bd_['count']\n",
    "            start_time = bd_[0]\n",
    "            end_time = bd_[1]\n",
    "            duration = anno[fname]['duration']\n",
    "            start_time = float(start_time) / float(duration)\n",
    "            end_time = float(end_time) / float(duration)\n",
    "            # print('file name', fname, 'bd_info', bd_info)\n",
    "            feat_file = os.path.join(save_path, str(cnt)+'.npy')\n",
    "            # print('fname', fname)\n",
    "\n",
    "            # if os.path.exists(feat_file):\n",
    "            #     continue\n",
    "            # vid = imageio.get_reader(os.path.join(file_path, fname + '.mp4'), 'ffmpeg')\n",
    "            # curr_frames = []\n",
    "            # for frame in vid:\n",
    "            #     if len(frame.shape)<3:\n",
    "            #         frame = np.repeat(frame,3)\n",
    "            #     curr_frames.append(transform(frame).unsqueeze(0))\n",
    "            # fr_cnt = len(curr_frames) - 1\n",
    "            # # print('curr_frame count', len(curr_frames))\n",
    "            # # start frame / end frame\n",
    "            # st_fr = fr_cnt * start_time\n",
    "            # end_fr = fr_cnt * end_time\n",
    "            # # print('start frame, end frame', st_fr, end_fr)\n",
    "            # curr_frames = torch.cat(curr_frames, dim=0)\n",
    "            # # print(\"Shape of frames: {0}\".format(curr_frames.shape))\n",
    "            # # get it by linspace, and rounding if the count of frames is smaller than sampling amount\n",
    "            # idx = np.linspace(st_fr, end_fr, frame_num) # .astype(int)\n",
    "            # idx = np.round(idx).astype(int)\n",
    "\n",
    "            # for idx_, i in enumerate(idx):\n",
    "            # \tif i >= len(curr_frames):\n",
    "            # \t\tidx[idx_] = len(curr_frames) - 1\n",
    "\n",
    "            # curr_frames = curr_frames[idx,:,:,:].cuda()\n",
    "            # # print(\"Captured {} frames: {}\".format(frame_num, curr_frames.shape))\n",
    "            # # print('frame count', len(curr_frames))\n",
    "            # curr_feats = []\n",
    "            # for i in range(0, frame_num, batch_size):\n",
    "            #     curr_batch = curr_frames[i:i+batch_size,:,:,:]\n",
    "            #     out = net(curr_batch)\n",
    "            #     curr_feats.append(out.detach().cpu())\n",
    "            #     # print(\"Appended {} features {}\".format(i+1,out.shape))\n",
    "            # curr_feats = torch.cat(curr_feats, 0)\n",
    "            # print(curr_feats.shape)\n",
    "            # del out\n",
    "            # np.save(feat_file,curr_feats.numpy())\n",
    "            # print(\"Saved file {}\\nExiting\".format(fname[:-4] + '.npy'))\n",
    "            # if fname[:-4] == '138LG':\n",
    "            # \tbreak\n",
    "\n",
    "            cnt += 1\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5336/5336 [00:00<00:00, 387439.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inceptionresnetv2 Network loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    " # /saat/Charades_for_SAAT\n",
    "# parser.add_argument('--dataset_name', type=str, default='Charades') # Charades\n",
    "\n",
    "# parser.add_argument('--start_idx', type=int, default=0)\n",
    "# parser.add_argument('--end_idx', type=int, default=1)\n",
    "# parser.add_argument('--batch_size', type=int, default=28)\n",
    "# parser.add_argument('--start', type=int)\n",
    "# parser.add_argument('--end', type=int)\n",
    "\n",
    "# opt = parser.parse_args()\n",
    "with open('/saat/charades_train_mapping_for_vid_cap.json', 'r') as f:\n",
    "\tanno_info = json.load(f)\n",
    "batch_size = 28\n",
    "dataset_name = 'Charades'\n",
    "anno, train_keys = anno_info, list(anno_info.keys())\n",
    "file_path = '/saat/Charades_for_SAAT'\n",
    "expnum = 'python'\n",
    "frame_per_video = 28\n",
    "\n",
    "save_path = os.path.join(file_path, 'Feature_2D_{}'.format(expnum))\n",
    "# if not os.path.exists(save_path):\n",
    "# \tos.makedirs(save_path)\n",
    "# namelist = os.listdir(os.path.join(opt.file_path, opt.dataset_name))\n",
    "# namelist_to_pass = []\n",
    "\n",
    "# for id_ in namelist:\n",
    "# \tif id_[:-4] in train_keys:\n",
    "# \t\tnamelist_to_pass.append(id_)\n",
    "# namelist_to_pass = sorted(namelist_to_pass)\n",
    "# print(namelist_to_pass)\n",
    "\n",
    "read_in_path = os.path.join(file_path, dataset_name)\n",
    "# extract_feats(read_in_path, train_keys, opt.frame_per_video, opt.batch_size, save_path, anno, opt.start, opt.end)\n",
    "cnt = extract_feats(read_in_path, train_keys, frame_per_video, batch_size, save_path, anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9190"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "883"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp3 = '/saat/charades_train_mapping_for_vid_cap_2000.json'\n",
    "with open(tmp3, 'r') as f:\n",
    "    tmp1n = json.load(f)\n",
    "len(list(tmp1n.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "847"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp3 = '/saat/charades_train_mapping_for_vid_cap_2000_4000.json'\n",
    "with open(tmp3, 'r') as f:\n",
    "    tmp1n = json.load(f)\n",
    "len(list(tmp1n.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "866"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp3 = '/saat/charades_train_mapping_for_vid_cap_4000_6000.json'\n",
    "with open(tmp3, 'r') as f:\n",
    "    tmp1n = json.load(f)\n",
    "len(list(tmp1n.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
